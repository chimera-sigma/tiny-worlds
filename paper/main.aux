\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{wei2022emergent}
\citation{garg2022what}
\citation{jaeger2001echo}
\citation{maass2002real}
\citation{jaeger2004harnessing}
\citation{alain2017understanding}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{jaeger2001echo}
\citation{jaeger2004harnessing}
\citation{maass2002real}
\citation{lukovsevivcius2009survey}
\citation{bertschinger2004real}
\citation{legenstein2007edge}
\citation{alain2017understanding}
\citation{hewitt2019structural}
\citation{hochreiter1997long}
\citation{cho2014learning}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Reservoir computing and echo state networks.}{2}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Edge of chaos and dynamical regimes.}{2}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Probes and internal representations.}{2}{section*.3}\protected@file@percent }
\citation{wei2022emergent}
\citation{garg2022what}
\@writefile{toc}{\contentsline {paragraph}{Gated recurrent architectures.}{3}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Emergence in large models.}{3}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Synthetic worlds and models}{3}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Worlds}{3}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{E3: Oscillator world.}{3}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{E2: Regime world.}{3}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{E1: Drift world.}{3}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Models and training}{3}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Evaluation metrics and statistical tests}{4}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Predictive gain.}{4}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Permutation tests.}{4}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Linear probes.}{4}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Local Jacobian analysis}{4}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{4}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Capacity and emergence in the oscillator world}{4}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Capacity curves for E3 oscillator world. Both GRU and RNN saturate by $H=8$. The GRU at $H=2$ achieves approximately 90\% of the $H=32$ performance, demonstrating that emergence does not require large hidden states when capacity matches latent dimensionality.}}{5}{figure.caption.12}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:capacity}{{1}{5}{Capacity curves for E3 oscillator world. Both GRU and RNN saturate by $H=8$. The GRU at $H=2$ achieves approximately 90\% of the $H=32$ performance, demonstrating that emergence does not require large hidden states when capacity matches latent dimensionality}{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Geometry versus dynamics: readout only and scrambled cores}{5}{subsection.4.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces E3 Oscillator: Capacity sweep for GRU and RNN (full training mode).}}{6}{table.caption.13}\protected@file@percent }
\newlabel{tab:e3_capacity}{{1}{6}{E3 Oscillator: Capacity sweep for GRU and RNN (full training mode)}{table.caption.13}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces E3 Oscillator at $H=16$: Comparison of training modes.}}{6}{table.caption.16}\protected@file@percent }
\newlabel{tab:e3_modes}{{2}{6}{E3 Oscillator at $H=16$: Comparison of training modes}{table.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Hidden dynamics: from Jacobians to Lyapunov spectra}{6}{subsection.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Probe $R^2$ versus capacity for E3 oscillator. Geometric representation saturates faster than predictive gain. The GRU achieves $R^2 > 0.97$ even at $H=2$; the RNN struggles at $H=2$ but matches GRU by $H=4$. This confirms that geometry is largely free once minimal capacity is met.}}{7}{figure.caption.14}\protected@file@percent }
\newlabel{fig:probe_capacity}{{2}{7}{Probe $R^2$ versus capacity for E3 oscillator. Geometric representation saturates faster than predictive gain. The GRU achieves $R^2 > 0.97$ even at $H=2$; the RNN struggles at $H=2$ but matches GRU by $H=4$. This confirms that geometry is largely free once minimal capacity is met}{figure.caption.14}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Local Jacobian statistics on the oscillator world E3 at $H = 16$. Mean and standard deviation of $\log |\det J_t|$ over sampled hidden states.}}{7}{table.caption.17}\protected@file@percent }
\newlabel{tab:jacobian_e3}{{3}{7}{Local Jacobian statistics on the oscillator world E3 at $H = 16$. Mean and standard deviation of $\log |\det J_t|$ over sampled hidden states}{table.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Geometry versus dynamics at $H=16$. \textbf  {Left:} Predictive performance ($\Delta $NLL). Scrambled cores fail catastrophically despite preserving geometric structure. The GRU readout-only achieves half of full performance; the RNN readout-only fails. \textbf  {Right:} Probe $R^2$ for latent decoding. All modes maintain high geometric decodability, confirming that geometry is free but dynamics must be learned.}}{8}{figure.caption.15}\protected@file@percent }
\newlabel{fig:modes}{{3}{8}{Geometry versus dynamics at $H=16$. \textbf {Left:} Predictive performance ($\Delta $NLL). Scrambled cores fail catastrophically despite preserving geometric structure. The GRU readout-only achieves half of full performance; the RNN readout-only fails. \textbf {Right:} Probe $R^2$ for latent decoding. All modes maintain high geometric decodability, confirming that geometry is free but dynamics must be learned}{figure.caption.15}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Lyapunov spectrum analysis on E3 oscillator at $H=16$. Values are mean $\pm $ 95\% CI over 5 seeds. The leading exponent $\lambda _1$ measures stability along the flow direction; $\lambda _{>1}$ mean measures transverse contraction. The difference in $\lambda _1$ between trained GRU and RNN is statistically significant ($p = 0.001$, two-sample $t$-test).}}{8}{table.caption.18}\protected@file@percent }
\newlabel{tab:lyapunov}{{4}{8}{Lyapunov spectrum analysis on E3 oscillator at $H=16$. Values are mean $\pm $ 95\% CI over 5 seeds. The leading exponent $\lambda _1$ measures stability along the flow direction; $\lambda _{>1}$ mean measures transverse contraction. The difference in $\lambda _1$ between trained GRU and RNN is statistically significant ($p = 0.001$, two-sample $t$-test)}{table.caption.18}{}}
\@writefile{toc}{\contentsline {paragraph}{Coasting experiments.}{8}{section*.19}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Architectural determinism and hallucination. Hidden state dynamics when trained on the Null World (pure noise). \textbf  {Top row (GRU):} Despite the lack of temporal structure in the input, the GRU hallucinates a coherent, high-amplitude oscillation. Its ``resonant'' prior ($\lambda _1 \approx 0$) forces the noise into a limit cycle. The Lyapunov spectrum (right) shows the characteristic High-Q signature even on null data. \textbf  {Bottom row (RNN):} The RNN correctly identifies the lack of signal, tracking the input noise with lower amplitude and no coherent phase structure. This demonstrates that the GRU's emergence is the result of an intrinsic inductive bias toward oscillation.}}{9}{figure.caption.21}\protected@file@percent }
\newlabel{fig:hallucination}{{4}{9}{Architectural determinism and hallucination. Hidden state dynamics when trained on the Null World (pure noise). \textbf {Top row (GRU):} Despite the lack of temporal structure in the input, the GRU hallucinates a coherent, high-amplitude oscillation. Its ``resonant'' prior ($\lambda _1 \approx 0$) forces the noise into a limit cycle. The Lyapunov spectrum (right) shows the characteristic High-Q signature even on null data. \textbf {Bottom row (RNN):} The RNN correctly identifies the lack of signal, tracking the input noise with lower amplitude and no coherent phase structure. This demonstrates that the GRU's emergence is the result of an intrinsic inductive bias toward oscillation}{figure.caption.21}{}}
\@writefile{toc}{\contentsline {paragraph}{The hallucination test.}{9}{section*.20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Discrete latent emergence in the regime world}{9}{subsection.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces E2 regime world: structured versus null. \textbf  {Left:} Predictive gain ($\Delta $NLL). Both architectures achieve large gains in the structured world but fail in the null world. \textbf  {Right:} Regime probe $R^2$. Unlike E3, representation quality differs between architectures (GRU $>$ RNN) and is absent in the null world. In E2, both geometry and dynamics must be learned.}}{10}{figure.caption.22}\protected@file@percent }
\newlabel{fig:e2}{{5}{10}{E2 regime world: structured versus null. \textbf {Left:} Predictive gain ($\Delta $NLL). Both architectures achieve large gains in the structured world but fail in the null world. \textbf {Right:} Regime probe $R^2$. Unlike E3, representation quality differs between architectures (GRU $>$ RNN) and is absent in the null world. In E2, both geometry and dynamics must be learned}{figure.caption.22}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces E2 Regime World at $H=16$: Structured vs Null.}}{10}{table.caption.23}\protected@file@percent }
\newlabel{tab:e2_results}{{5}{10}{E2 Regime World at $H=16$: Structured vs Null}{table.caption.23}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{10}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Inductive resonance vs.\ generic contraction.}{10}{section*.24}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The double-edged sword of architectural priors.}{11}{section*.25}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mechanism over scale.}{11}{section*.26}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Geometry is free, dynamics are expensive.}{11}{section*.27}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{On the definition of emergence.}{11}{section*.28}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Limitations and future work.}{11}{section*.29}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{12}{section.6}\protected@file@percent }
\bibcite{wei2022emergent}{1}
\bibcite{garg2022what}{2}
\bibcite{jaeger2001echo}{3}
\bibcite{jaeger2004harnessing}{4}
\bibcite{maass2002real}{5}
\bibcite{lukovsevivcius2009survey}{6}
\bibcite{bertschinger2004real}{7}
\bibcite{legenstein2007edge}{8}
\bibcite{alain2017understanding}{9}
\bibcite{hewitt2019structural}{10}
\bibcite{hochreiter1997long}{11}
\bibcite{cho2014learning}{12}
\gdef \@abspage@last{13}
